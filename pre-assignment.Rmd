---
title: "pre-assignment"
author: "WoodyLIver"
date: "27 04 2021"
output:
  html_document: default
  pdf_document: default
---
# Libraries
```{r}
library(openxlsx)
library(tseries)
library(forecast)
library(EnvStats)
library(lmtest)
library(sigmoid)
```

# Read data
```{r cars}
data <- read.xlsx("Lions_Den_data.xlsx")
ts <- ts(unlist(data[2]), start=c(1994, 7), frequency=12)
plot(ts)
```

# Decomposition
```{r}
decompose <- decompose(ts, "additive")
plot(decompose)
```

# Outliers
Test on raw data that the winter 2006 does not match. We have tried to manipulate data(differentiation, decomposition, etc) in order to get more outliers, but there were no reasonable results.
```{r}
plot(ts, type="o")
rosnerTest(ts, k = 3)
```
# Autocorrelation and stationarity analysis
We can see that time-series is already stationary and is slightly correlated with itself 12 month earlier.
```{r}
#autocorrelation
acf(ts, lag.max = 12)
#stationary test
adf.test(ts)
```
## PREDICTION
# Delete outlier
```{r}
ts_ <- ts
plot(window(ts_, 2003, 2008), type="l", ylab="coal usage")
ts_[137:145] = ts_[137:145] * 0.6
lines(window(ts_, 2003, 2008), type="l", col="blue")
```
\p

# Create model
Model performs decomposition into trend and seasonality, then trend is predicted by ARIMA.
Result is a sum of trend prediction and seasonality pushed through relu.
```{r}
model <- function (ts) {
  decompose_ <- decompose(ts, "additive")
  tsTrend <- ts(decompose_$trend, start=ts, frequency=12)
  fitARIMA <- arima(tsTrend, order=c(1,1,1),seasonal = list(order = c(1,0,0), period = 12),method="ML")
  yTrend <- predict(fitARIMA,n.ahead = 12)$pred
  cTrend <- ts(
    c(tsTrend, yTrend),
    start=start(tsTrend),
    frequency=12
  )
  plot(cTrend, type="s", ylab="coal usage trend")
  ySeasonal <- window(decompose$seasonal, start=end(ts)[1], end=end(ts))
  Y <- ts(as.numeric(yTrend) + as.numeric(ySeasonal),
        start=start(ySeasonal)[1]+1,
        frequency=12)
  Y <- relu(Y)
  TS <- ts(c(ts, Y), start=start(ts), frequency=12)
  plot(TS, type="l", ylab="coal usage",
       main="history and prediction", col="black")
  lines(Y, col="blue")
  print(Y)
  return(Y)
}
```
# Predict
```{r}
model(ts_)
```
\p

#Evaluate
Both, plot and acuracy test shows that our model performs much better than naive method.
```{r}
arg <- window(ts_, end=c(2019,12))
trg <- window(ts_, start=2020)
ret <- model(arg)
naive <- ts(rep(mean(arg), 12), start=2020, frequency=12)
plot(trg)
lines(ret, col="blue")
lines(naive, col="green")
accuracy(naive, trg)
accuracy(ret, trg)
```
\p

#Additional Variables
Knowing the characteristic of the series, itâ€™s seasonality and known specific of coal consumption we can improve our model adding variables of temperature (it is known that higher temperature affects lower coal consumption, also for energy production and building heating) in our country and the share of coal in our energetic mix which can affect future coal consumption. Other variable may be coal price (coal supply might play the same role), but it will somehow correlated with energetic mix and depends on our possibility to swich power source. 
